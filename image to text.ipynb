{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "describe the mood and colors of this image are a bit of warm\n",
      "arafed woman wearing a sweater and scarf standing in front of a building\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "\n",
    "# url로 열기\n",
    "# img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "# raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "# 파일 직접 열기\n",
    "raw_image = Image.open(\"f330a80912b0a0f3.jpg\").convert('RGB')\n",
    "\n",
    "# conditional image captioning\n",
    "text = \"describe the mood and colors of this image\"\n",
    "inputs = processor(raw_image, text, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "# unconditional image captioning\n",
    "inputs = processor(raw_image, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1번 프롬프트\n",
    "import os\n",
    "from PIL import Image\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI API 키 설정\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# BLIP 모델 설정\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "\n",
    "# 이미지 처리\n",
    "raw_image = Image.open(\"unnamed.jpg\").convert('RGB')\n",
    "text = \"the image colors of\"\n",
    "inputs = processor(raw_image, text, return_tensors=\"pt\")\n",
    "out = model.generate(**inputs)\n",
    "blip_caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(blip_caption)\n",
    "\n",
    "# Langchain ChatGPT 설정\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    openai_api_key=os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "# 프롬프트 작성\n",
    "prompt = f\"\"\"\n",
    "다음 이미지 설명을 감성적이고 색감 위주의 설명으로 바꿔주세요:\n",
    "인물이 있으면 인물을 중점적으로 설명하고 간결하게 포인트를 중점으로 설명해주세요\n",
    "배경이면 그 배경의 색감과 공간에 위치에 대한 느낌을 중점으로 설명\n",
    "{blip_caption}\n",
    "\n",
    "예시)\n",
    "\"인물이라면 인물이 입고있는 옷에 대한 느낌과 그 색깔에 대한 느낌\"\n",
    "\"공간이라면 전체적인 공간의 색감과 느낌, 공간에 대한 느낌\"\n",
    "\"\"\"\n",
    "\n",
    "# Langchain을 사용하여 GPT 응답 생성\n",
    "response = chat([HumanMessage(content=prompt)])\n",
    "emotional_description = response.content\n",
    "print(emotional_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20118\\miniforge3\\envs\\langchain_env\\Lib\\site-packages\\transformers\\generation\\utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the image colors of a woman is taking a selfie in a mirror\n",
      "이미지 속 여성은 거울 앞에서 셀카를 찍고 있으며, 그녀의 존재감은 색감과 분위기로 더욱 돋보입니다. \n",
      "\n",
      "1. 주요 색상들은 따뜻한 핑크와 부드러운 화이트가 어우러져, 사랑스럽고 친근한 무드를 만들어냅니다. 이 조합은 자아에 대한 애정과 경쾌함을 전달합니다. \n",
      "2. 인물이 입은 의상은 연한 파스텔 톤으로, 부드러운 감정을 자아내며, 경쾌하면서도 차분한 느낌을 줍니다. 이는 그녀의 개성과 자신감을 표현합니다.\n",
      "3. 피부톤은 자연스러운 따뜻함을 띠고 있으며, 주변 조명은 부드럽고 은은해, 아늑한 분위기를 조성합니다. 이 조명은 그녀의 아름다움을 더욱 강조합니다.\n",
      "4. 배경색은 미세하게 변하는 그라데이션으로, 인물과의 조화가 잘 이루어져 있습니다. 이러한 조화는 전체적인 감성을 한층 풍부하게 만듭니다.\n",
      "5. 가장 눈에 띄는 색상 포인트는 인물의 의상 속 밝은 포인트 컬러로, 시선을 끌며 생동감을 더해줍니다. 이는 전체적인 이미지에 활력을 불어넣습니다.\n",
      "\n",
      "이러한 요소들이 함께 어우러져, 이미지 속 인물의 순간이 더욱 특별하고 감성적으로 느껴지게 합니다.\n"
     ]
    }
   ],
   "source": [
    "# 2번 프롬프트\n",
    "import os\n",
    "from PIL import Image\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI API 키 설정\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# BLIP 모델 설정\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "\n",
    "# 이미지 처리\n",
    "raw_image = Image.open(\"다운로드.jfif\").convert('RGB')\n",
    "text = \"the image colors of\"\n",
    "inputs = processor(raw_image, text, return_tensors=\"pt\")\n",
    "out = model.generate(**inputs)\n",
    "blip_caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(blip_caption)\n",
    "\n",
    "# Langchain ChatGPT 설정\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    openai_api_key=os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "# 프롬프트 작성\n",
    "prompt = f\"\"\"\n",
    "다음 이미지에 대한 설명을 색감과 분위기를 중심으로 재해석해주세요:\n",
    "{blip_caption}\n",
    "\n",
    "이미지에 인물이 있다면:\n",
    "1. 주요 색상들의 조합과 그들이 만들어내는 전반적인 무드\n",
    "2. 인물이 입은 의상의 색상이 주는 감정적 느낌\n",
    "3. 피부톤과 주변 조명이 만드는 분위기\n",
    "4. 배경색과 인물의 조화가 전달하는 감성\n",
    "5. 가장 눈에 띄는 색상 포인트와 그 효과\n",
    "\n",
    "이미지가 공간/풍경이라면:\n",
    "1. 전체적인 색감의 톤과 무드\n",
    "2. 가장 지배적인 색상과 그것이 주는 감정적 효과\n",
    "3. 공간 내 색상의 그라데이션이나 변화\n",
    "4. 조명이나 그림자가 만드는 색감의 변주\n",
    "5. 보완색이나 대비되는 색상들의 조화\n",
    "6. 인물이 있을때를 가정하지 않는다.\n",
    "\n",
    "위 요소들을 포함해 자연스럽고 감성적인 문체를 짧게 작성해주세요. \n",
    "\"\"\"\n",
    "\n",
    "# Langchain을 사용하여 GPT 응답 생성\n",
    "response = chat([HumanMessage(content=prompt)])\n",
    "emotional_description = response.content\n",
    "print(emotional_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20118\\miniforge3\\envs\\langchain_env\\Lib\\site-packages\\transformers\\generation\\utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the image colors are red and black and the woman is taking a selfie\n",
      "붉은 색과 검은 색이 어우러진 이 이미지는 강렬하면서도 신비로운 분위기를 자아냅니다. 특히 붉은 색이 주는 열정과 에너지가 눈에 띄며, 검은 색은 그 반대편에서 깊이와 세련됨을 더해줍니다. 이러한 색조의 조화는 관람자에게 강렬한 감정을 불러일으키며, 상반된 감정의 대립을 통해 더욱 깊은 여운을 남깁니다.\n"
     ]
    }
   ],
   "source": [
    "# 3번 프롬프트\n",
    "import os\n",
    "from PIL import Image\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI API 키 설정\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# BLIP 모델 설정\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "\n",
    "# 이미지 처리\n",
    "raw_image = Image.open(\"다운로드.jfif\").convert('RGB')\n",
    "text = \"the image colors are\"\n",
    "inputs = processor(raw_image, text, return_tensors=\"pt\")\n",
    "out = model.generate(**inputs)\n",
    "blip_caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(blip_caption)\n",
    "\n",
    "# Langchain ChatGPT 설정\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    openai_api_key=os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "# 프롬프트 작성\n",
    "prompt = f\"\"\"\n",
    "다음 이미지에 대한 설명을 색감과 분위기를 중심으로 재해석 해주세요\n",
    "사진에 나온 사실만 설명해주세요:\n",
    "{blip_caption}\n",
    "\n",
    "이미지에 인물이 있다면:\n",
    "1. 주요 색상들의 조합과 그들이 만들어내는 전반적인 무드\n",
    "2. 인물이 입은 의상의 색상이 주는 감정적 느낌\n",
    "3. 가장 눈에 띄는 색상 포인트와 그 효과\n",
    "4. 옷에 패턴이 있다면 그 느낌도 표현\n",
    "5. 인물의 표정과 포즈가 있다면 제외\n",
    "6. 인물에 대한 설명 제외\n",
    "\n",
    "이미지가 공간/풍경이라면:\n",
    "1. 전체적인 색감의 톤과 무드\n",
    "2. 가장 지배적인 색상과 그것이 주는 감정적 효과\n",
    "3. 인물이 있을때를 가정하지 않는다.\n",
    "4. 사진에 풍경에 대한 설명을 추가\n",
    "5. 사진에 포함된 요소들이 주는 느낌 추가\n",
    "\n",
    "위 요소들을 포함해 자연스럽고 감성적인 문체를 3줄이내로 작성해주세요. \n",
    "\"\"\"\n",
    "\n",
    "# Langchain을 사용하여 GPT 응답 생성\n",
    "response = chat([HumanMessage(content=prompt)])\n",
    "emotional_description = response.content\n",
    "print(emotional_description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
